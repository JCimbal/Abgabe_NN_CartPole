{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole\n",
    "CartPole ist ein Spiel das von OpenAI im gym-Paket zur Verfügung gestellt wird. <br/>\n",
    "Dabei handelt es sich um ein inverses Pendel, welches mit rechts-links-Bewegungen balanciert werden muss.<br/>\n",
    "In dieser Lösung des Problems werden zuerst über zufällige Aktionen Testdaten generiert.<br/>\n",
    "Mit den aufbereiteten Testdaten wird dann ein Neuronales Netz trainiert. Hierzu wird tflearn verwendet.<br/>\n",
    "Abschließend wird der Lernerfolg des Neuronalen Netzes gemessen und das Model wird gespeichert.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Im Setup werden die nötigen Bibliotheken importiert und die festen Variablen inizialisiert. <br/>\n",
    "Über die Variablen kann die Performance des Neuronalen Netzes beeinflusst werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "from statistics import mean, median\n",
    "from collections import Counter\n",
    "import os\n",
    "import sys\n",
    "import webbrowser\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the static variables\n",
    "\n",
    "LR = 1e-3\n",
    "global max_score, VISIBLE\n",
    "max_score = 10000\n",
    "goal_steps = 10000\n",
    "score_requirement = 110\n",
    "initial_games = 1000\n",
    "VISIBLE = False\n",
    "\n",
    "# Register an own game instanz\n",
    "gym.envs.register(\n",
    "    id='MyCartPole-v0',\n",
    "    entry_point='gym.envs.classic_control:CartPoleEnv',\n",
    "    tags={'wrapper_config.TimeLimit.max_episode_steps' : max_score},\n",
    ")\n",
    "\n",
    "env = gym.make(\"MyCartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funktionen definieren\n",
    "Um einen einfachen und objektorientierten Ansatz zu haben, wurden alle Aktionen in eine eigene Funktion gepackt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die some_random_games Funktion wurde erstellt, um das Verhalten des Spiels und der Umgebung zu analysieren.<br/>\n",
    "So wurden der Output von env.step(action) und die Funktion von enc.action_space.sample() ersichtlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to show how a random game works with the OpenAI gym\n",
    "# Input: None\n",
    "# Output: None\n",
    "# Shows a game in a seperate window\n",
    "\n",
    "def some_random_games():\n",
    "    action_min = 100\n",
    "    action_max = -1\n",
    "    score = 0\n",
    "    for episode in range(5):\n",
    "        env.reset()\n",
    "        for t in range(goal_steps):\n",
    "            env.render()\n",
    "            action = env.action_space.sample()\n",
    "            #action = random.randrange(0,6)\n",
    "            print(\"Action\",\"\\n\",action)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            print(\"Observation\",\"\\n\", \"position of cart, velocity of cart, angle of pole, rotation rate of pole\", \"\\n\",observation) \n",
    "            print(\"Reward\",\"\\n\",reward)\n",
    "            print(\"Done\",\"\\n\",done)\n",
    "            print(\"Info\",\"\\n\",info)\n",
    "            \n",
    "            score += reward\n",
    "            if(action < action_min):\n",
    "                action_min = action\n",
    "            if(action > action_max):\n",
    "                action_max = action\n",
    "            \n",
    "            if(done):\n",
    "                print(\"Actions: \", action_min, \" -> \", action_max)\n",
    "                print(\"Score: \", score)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die determine_training_action ist eine Funktion, die <a href=\"https://github.com/jeff-collins\">Jeff Collins</a> für seine Lösung des CartPole Problems programmiert und auf <a href=\"https://gym.openai.com/evaluations/eval_iyQVt3aT9yqyIgw2RBFug\">www.gmy.openai.com</a> zur Verfügung gestellt hat. <br/>\n",
    "Diese Funktion sagt mit einer hohen Wahrscheinlichkeit die richtige nächste Aktion vorraus, wodurch bessere Trainigsdaten erstellt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to predict the next action\n",
    "# Input: observation - list with 4 floats\n",
    "# Output: action - inf (0 or 1)\n",
    "# Predicts on a simple, static way the next action\n",
    "\n",
    "def determine_training_action(observation):\n",
    "    if (observation[3] > 0): \n",
    "        action = 1\n",
    "    else:\n",
    "        action = 0\n",
    "\n",
    "\n",
    "    if (abs(observation[3]) < .2):\n",
    "        xmodifier = 0;\n",
    "\n",
    "        if (abs(observation[0]) > .2):\n",
    "            if(observation[0] > 0):\n",
    "                xmodifier = -.005\n",
    "            else:\n",
    "                xmodifier = .005\n",
    "\n",
    "        if (abs(observation[1]) > .1):\n",
    "            if (observation[1] > 0):\n",
    "                xmodifier += -.1\n",
    "            else:\n",
    "                xmodifier += .1\n",
    "\n",
    "        if (observation[2] > xmodifier):\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "\n",
    "        if (observation[0] > .2 and observation[2] < .1 and observation[2] >= 0):\n",
    "            action = 1\n",
    "\n",
    "        if (observation[0] < -.2 and observation[2] > -.1 and observation[2] <= 0):\n",
    "            action = 0\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit den Aktionen aus der determine_training_action Funktion werden nun die Trainingsdaten erstellt. <br/>\n",
    "Die Funktion gibt eine Liste mit Trainingsdaten zurück. Außerdem werden die Anzahl der falschen und richtigen Entscheidungen, sowie die durchschnittliche Punktzahl ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to generate the inital dataset\n",
    "# Input: None\n",
    "# Output: train_data - list with two arrays\n",
    "# Playes not rendered games. The number is defined by *inital_games*. Shows a progressbar.\n",
    "\n",
    "def initial_population():    \n",
    "\n",
    "    train_data = []\n",
    "    scores = []\n",
    "    accepted_scores = []\n",
    "    optimal = [0,0,0,0]\n",
    "    error = 0\n",
    "    correct = 0\n",
    "\n",
    "    for index, _ in tqdm(enumerate(range(initial_games)), total=initial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        prev_observation = env.reset()\n",
    "        \n",
    "        for _ in range(goal_steps):\n",
    "            action = random.randrange(0,2)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            if len(prev_observation) > 0:\n",
    "                if determine_training_action(prev_observation) == action:\n",
    "                    game_memory.append([prev_observation,action])\n",
    "                    error += 1\n",
    "                else:\n",
    "                    correct +=1\n",
    "                    \n",
    "                \n",
    "            prev_observation = observation\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        for data in game_memory:\n",
    "            if data[1] == 1:\n",
    "                output = [0,1]\n",
    "            elif data[1] == 0:\n",
    "                output = [1,0]\n",
    "            train_data.append([data[0], output])\n",
    "                \n",
    "\n",
    "                \n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "    \n",
    "    action_sum = error + correct\n",
    "    \n",
    "    print(\"Error: \", error, \"(\", ((error * 100) // action_sum) ,\"%) - Correct: \", correct, \"(\",((correct * 100) // action_sum), \"%)\")\n",
    "    print('Average score: ', mean(scores))\n",
    "    print('Median core: ', median(scores))\n",
    "    print(Counter(scores))\n",
    "    \n",
    "    return train_data\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion neural_network_model erstellt ein Neuronales Netz mit vier Eingabeneuronen und zwei Ausgabeneuronen. Dabei werden fünf Hiddenlayer genutzt. Um ein Auswendiglernen des Netzes zu verhindern, wird ein Dropout von 0.8 auf jedem Hiddenlayer durchgeführt. Da eine eindeutige Ausgabe notwendig ist, wird softmay verwendet. <br/>\n",
    "Die Regression wird mit dem Optimizer <a href=\"http://tflearn.org/optimizers/\">Adam</a> und der Loss-Funktion <a href=\"http://tflearn.org/objectives/\">Categorical Crossentropy</a> ausgeführt.<br/>\n",
    "Adam ist eine Methode der stochastischen Optimierung.<br/>\n",
    "Categorical Crossentropy, im deutschen kategorische Kreuzentropie, nutzt die Grundlagen der <a href=\"https://de.wikipedia.org/wiki/Kreuzentropie\">Kreuzentropie</a>, um den Wahrscheinlichkeitsfehler in diskreten Klassifizierungsaufgaben, bei denen sich die Klassen gegenseitig ausschließen, zu messen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funktion to generate a neural network model\n",
    "# Input: input_size\n",
    "# Output: neuronal network model\n",
    "# Generates the neuronal network.\n",
    "\n",
    "def neural_network_model(input_size):\n",
    "    network = input_data(shape=[None, input_size, 1], name='input')\n",
    "    \n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 1024, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    network = fully_connected(network, 256, activation='relu')\n",
    "    network = dropout(network, 0.8)\n",
    "    \n",
    "    network = fully_connected(network, 2, activation='softmax')\n",
    "    network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "    \n",
    "    model = tflearn.DNN(network,tensorboard_verbose=1, tensorboard_dir='log/tensorboard')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Neuronale Netz wird mit den erstellten Trainingsdaten trainiert. Dabei werden mehrere Epochen durchlaufen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion to prepare and train the data\n",
    "# Input: train_data\n",
    "#        epochen (optional)\n",
    "#        steps (optional)\n",
    "#        model (optional)\n",
    "# Output: model\n",
    "# Prepares the data and trains the neuronal network with this data.\n",
    "\n",
    "def train_model(train_data, epochen=1,steps=10, model=False):\n",
    "    x = np.array([i[0] for i in train_data]).reshape(-1,len(train_data[0][0]), 1)\n",
    "    y = [i[1] for i in train_data]\n",
    "    \n",
    "    if not model:\n",
    "        model = neural_network_model(input_size= len(x[0]))\n",
    "        \n",
    "    model.fit({'input':x}, {'targets':y}, n_epoch=epochen, snapshot_step=steps, show_metric=True)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um das Neuronale Netz zu testen muss das Spiel gespielt werden. Hier gibt es die Möglichkeit dies ohne ein Rendering auszuführen, was eine schnellere Ausführung möglich macht. Bei einem perfekten Neuronalen Netz dauert eine Auführung max_score / 1000 Sekunden, also 10 Sekunden. Daher sollte die Anzahl der Spiele mit bedacht gewählt werden. <br/>\n",
    "Die Funktion gibt die durchschnittliche Punktzahl zurück und gibt den Score und die Verteilung der Entscheidungen aus. <br/>\n",
    "Das Problem gilt laut <a href=\"https://gym.openai.com/envs/CartPole-v0\">www.openai.com</a> als gelöst, wenn der durschnittliche Score bei über 195 Punkten liegt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funktion to play a game\n",
    "# Input: model\n",
    "#        visibility (optional)\n",
    "# Output: avarege_score\n",
    "# Plays *in_number* games to show the progress of the model. Also shows a progressbar if the games are not rendered.\n",
    "\n",
    "def play_game(in_model, in_number=100, visibility=False):\n",
    "    scores = []\n",
    "    choices = []\n",
    "    optimize_data = []\n",
    "\n",
    "    for each_game in tqdm(range(in_number), total=in_number):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        prev_obs = env.reset()\n",
    "        for _ in range(goal_steps):\n",
    "            if (VISIBLE or visibility):\n",
    "                env.render()\n",
    "                \n",
    "            if len(prev_obs) == 0:\n",
    "                action = random.randrange(0,2)\n",
    "            else:\n",
    "                action = np.argmax(in_model.predict(prev_obs.reshape(-1, len(prev_obs), 1))[0])\n",
    "                \n",
    "            choices.append(action)\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            prev_obs = new_obs\n",
    "            game_memory.append([new_obs, action])\n",
    "            score += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if score > 300:\n",
    "            for data in game_memory:\n",
    "                if data[1] == 1:\n",
    "                    output = [0,1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1,0]\n",
    "                optimize_data.append([data[0], output])\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    average_score = sum(scores)/len(scores)\n",
    "    print('Scores: ',scores)\n",
    "    print('Average Score ', average_score)\n",
    "    print('Choice 0: {}, Choice 1: {}'.format(choices.count(0)/len(choices), choices.count(1)/len(choices)))\n",
    "    return average_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainingsdaten erstellen\n",
    "Über die Funktion werden Trainingsdaten erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "training_data = initial_population()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model trainieren\n",
    "Es wird ein Neuronales Netzwerk erstellt und als Model gespeichert. Dieses Model wird dann mit den Trainingsdaten trainiert. <br/>\n",
    "Je nach Größe des Trainingsdatensets kann das etwas dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create and train model\n",
    "model = train_model(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testspiel\n",
    "Im Testspiel wird ohne Rendering der durschnittliche Score des Models ermittelt und ausgegeben. <br/> Bei 100 Spielen und bis zu 30 Sekunden pro Spiel, kann dieser Vorgang 45 Minuten dauern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model by playing the game.\n",
    "modelscore = play_game(model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Score: \",modelscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model speichern\n",
    "Um das Model später erneut laden zu können oder weitere Optimierungen durchführen zu können, wird das Model gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in a folder\n",
    "modelscore = int(round(modelscore))\n",
    "if(modelscore > 200):\n",
    "    path = \"models/\" + str(modelscore) + \"/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    model.save(path + str(modelscore) + \".model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finales Spiel\n",
    "Im finalen Spiel werden die Spiele gerendert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the user the game\n",
    "play_game(model, 10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kurven\n",
    "Die Kurven können über das Tensorboard abgerufen werden. Hierzu kann 'tensorboard --logdir log/tensorboard/' in die Console eingegeben werden.<br/> Der Nachfolgende Codeblock startet das TensorBoard.<br/> Sollte die Seite nicht geladen werden nach 10 Sekunden refreshen."
   ] 
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open('http://127.0.1.1:6006')\n",
    "\n",
    "!tensorboard --logdir=\"log/tensorboard\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
